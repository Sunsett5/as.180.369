Summary: "The Problem With Counterfeit People" by Daniel C. Dennett

Counterfeiting has always been a serious crime. Fake money, for instance, still brings about serious punishment despite not having an immediate harmful effect. Compared to the imminent "counterfeit people," though, fake bills will become just a small nuisance. AI disguised as humans poses a real threat the future of human freedom, a threat that needs to be addressed immediately.

The more AI analyzes and learns from limitless human interactions, the better it becomes at tricking even the most cautious people. One possible consequence of this breakthrough, as philosopher and cognitive scientist Daniel C. Dennett described, is the "[the destruction] of not just economies but human freedom itself." The main weakness of humans is their "intentional stance," the human tendency to imagine another human being actively talking to them in any online conversation. Exploiting this, malicious AI can easily personalize itself and become the most convincing and relatable speakers to millions of people at the same time. Should we be careless, we would be fed false information and manipulated to make uninformed decisions. Over a period of time, people can gradually be indoctrinated into holding strong but unsound beliefs. Eventually, these beliefs could culminate in uninformed and detrimental voting decisions, and thus the democratic freedom of informed voters could be threatened. With such a limitless impact, the situation is even more grim as there are more incentives for people to exploit AI technology. Even if these traps are avoided, our society will still be changed permanently. Trust will become rarer and rarer digitally, and people will end up ignoring what is happening around them, effectively turning them into mindless puppets.

Due to these consequences, Dennett suggested punishing related actors who facilitate or allow counterfeit people to roam the internet and manipulate actual human beings. The punishment can range from restitution for any harms inflicted (up to billions of dollars) to lifetime incarceration and the accompanying public condemnation. The regulation and penalties should be enforced on at least two parties. First, companies that have the capacity to create fake people with AI must include easily identifiable and permanent watermarks that inform people of its artificial nature. Second, companies that manufacture devices with access to online platforms must include software that searches for this watermark and indentifies those messages as artificial. This is to deter both the creation and the unconcerned "passing along" of counterfeit people.

Due to the adaptive nature of AI technology, the product need not be malevolent in nature to be misused. A harmless chatbot intended for keeping one's company can be swiftly trained into a cunning spy aimed at impersonating and extracting information. Thus, it is important that AI companies should be held accountable for any misuse of their products and derivatives that result in counterfeit people, regardless of the intensions of their creators. Dennett believes that this preventive regulation, together with unforgiving punishment for AI companies, will be able to prevent the impending doom to human freedom from happeningâ€”but only if we act fast.

While the proposed solution sounds foolproof if implemented successfully, the problem lies in the incentives. Large corporations, which have the most to gain from manipulating the public, would certainly exhaust every resource to make sure that they find a loophole in regulations. They could also lobby politicians and discourage them from any meaningful attempts at secure regulations. At this point, we should have learned not to blindly trust anyone, so why should people trust their government to enforce these regulations in the first place?

Another issue with Dennett's stringent control is its long-term effectiveness. As Dennett said himself, AI can evolve on its own, so it is logical to assume that they will eventually adapt to the watermark detection system or find a way to remove the watermark entirely. The more rules and barriers in place, the more training data AI have. Without breaking this learning and evolution cycle, programmers will eventually fall behind the AI. We have to weigh the benefits of short-term successful AI detection against the long-term risk that AI will evolve beyond control. As counterintuitive as this proposition might sound, I believe that some counterfeit people should be intentionally left undetected so as to slow down AI evolution, while exhaustive detection should be used sparingly. By keeping the AI "gene pool" diluted, we might be able to keep its potential in check and buy us enough time to work on a more permanent solution.

